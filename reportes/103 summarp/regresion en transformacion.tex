\documentclass[11pt]{article}

% Idioma / codificación
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Matemáticas
\usepackage{amsmath}   % entornos de ecuaciones, \arg\min, \operatorname, etc.
\usepackage{amssymb}   % símbolos extra
\usepackage{amsfonts}
\usepackage{amsthm}    % entornos theorem/definition/lemma
\usepackage{bm}        % \bm para vectores/matrices en negritas
\usepackage{mathtools} % extensiones de amsmath (opcional)

% Hiperlinks
\usepackage{hyperref}

\begin{document}


\section{Descripción general del módulo \texttt{py103}: 
Tendencia penalizada para una serie de precios}

El objetivo del módulo es tratar una serie temporal de precios diarios
(p.\,ej.\ NVDA) como un problema de regresión en el tiempo,
aplicar varias transformaciones sobre el precio, ajustar una familia
de modelos de regresión penalizada (Elastic Net polinomial en el índice de tiempo)
y evaluar:

\begin{itemize}
  \item métricas de error no ponderadas y ponderadas (más peso a los días recientes),
  \item tanto en el espacio transformado como en el espacio original de precios,
  \item incluyendo diagnósticos de residuos (normalidad) y
  \item una selección de modelo que balancea ``buen RMSE'' y
        ``residuos lo más normales posible''.
\end{itemize}

A continuación se describe con detalle qué hace cada bloque del módulo
y por qué.

\subsection{Notación básica}

Sea
\[
  \{P_t\}_{t=0}^{T-1}
\]
la serie de precios de cierre diario de una acción/índice
(p.\,ej.\ NVDA), donde:
\begin{itemize}
  \item $t$ es un índice de tiempo entero (días consecutivos),
  \item $T$ es el número total de observaciones, y
  \item $P_t > 0$ es el precio de cierre en el día $t$.
\end{itemize}

Definimos también:
\[
  t \in \{0,1,\dots,T-1\}
  \quad\Rightarrow\quad
  x_t := t
\]
como la covariable (regresor) que representa el tiempo.

En el módulo:
\begin{itemize}
  \item \texttt{download\_price\_series} descarga los precios diarios
        desde Yahoo! Finance usando \texttt{yfinance} y devuelve
        un \texttt{DataFrame} con índice de fechas y una columna
        única \texttt{'close'} (la serie $\{P_t\}$).
  \item Usamos siempre un \emph{split temporal} $60/20/20$:
        \begin{align*}
        N_{\text{train}} &= \bigl\lfloor 0.6\,T \bigr\rfloor,\\
        N_{\text{val}}   &= \bigl\lfloor 0.2\,T \bigr\rfloor,\\
        N_{\text{test}}  &= T - N_{\text{train}} - N_{\text{val}},
        \end{align*}
        y los índices de cada segmento se obtienen con
        \texttt{\_segment\_indices} / \texttt{temporal\_split}.
\end{itemize}

\section{Transformaciones de la serie de precios}

\subsection{Transformaciones (precio respuesta)}

La idea central es no trabajar siempre directamente con $P_t$,
sino con una versión transformada
\[
  Y_t = g(P_{0:T-1})_t,
\]
que puede estabilizar la varianza o aproximar estacionariedad
(retornos, diferencias, etc.).
El módulo \texttt{py103} define:


y para las transformaciones que requieren casos escribimos por separado:


Todas estas transformaciones están registradas en el diccionario
\texttt{TRANSFORM\_FUNCS} y se aplican a un vector de precios
\texttt{prices}.

\subsection{Enmascaramiento y construcción de $(X,y,\text{dates})$}

\texttt{make\_transformed\_time\_regression\_data} hace:

\begin{enumerate}
  \item Extrae el vector de precios
        $\mathbf{p} = (P_0,\dots,P_{T-1})^\top$ de la columna
        \texttt{'close'}.
  \item Aplica $g$ para obtener
        $\mathbf{y}^{\text{raw}} = (Y_0,\dots,Y_{T-1})^\top$.
  \item Construye una máscara booleana
        \[
          m_t = \mathbf{1}\{ Y_t \text{ es finito} \},
        \]
        y la aplica tanto a $Y_t$ como a las fechas.
        Esto elimina los \texttt{NaN} introducidos por transformaciones
        como retornos o diferencias en $t=0$.
  \item Define la covariable de regresión:
        \[
          x_t = t, \quad t=0,\dots,N-1,
        \]
        donde $N$ es el número de puntos finitos restantes.
        El diseño queda como
        \[
          X = 
          \begin{bmatrix}
            0\\
            1\\
            \vdots\\
            N-1
          \end{bmatrix}
          \in\mathbb{R}^{N\times 1},
          \quad
          y = (Y_0,\dots,Y_{N-1})^\top.
        \]
  \item Devuelve $(X,y,\text{dates})$.
\end{enumerate}

En resumen: para cada transformación tenemos una serie
transformada $y_t$ y un índice temporal $x_t$ sobre el que haremos regresión.

\section{Modelo de regresión en el tiempo: Elastic Net polinomial}

\subsection{Modelo polinomial en el tiempo}

Para un grado polinomial $d\ge 1$, definimos el vector de características
\[
  \phi_d(x_t)
  = \bigl( x_t,\; x_t^2,\; \dots,\; x_t^d \bigr)^\top \in \mathbb{R}^d.
\]

El modelo de tendencia en el espacio transformado es:
\[
  y_t = \beta_0 + \beta^\top \phi_d(x_t) + \varepsilon_t,
  \quad t \in \mathcal{I}_{\text{train}},
\]
donde $\beta_0\in\mathbb{R}$, $\beta\in\mathbb{R}^d$
y $\varepsilon_t$ es el residuo.

En \texttt{build\_elastic\_net\_model} esto se implementa como:

\begin{center}
  \texttt{[ PolynomialFeatures(degree=d) ] $\rightarrow$ StandardScaler $\rightarrow$ ElasticNet}
\end{center}

con parámetros:
\begin{itemize}
  \item \texttt{alpha} $>0$ (intensidad de regularización),
  \item \texttt{l1\_ratio}\,$\in[0,1]$ (mezcla Lasso/Ridge),
  \item \texttt{poly\_degree} $= d$.
\end{itemize}

\subsection{Penalización Elastic Net}

Sea $\tilde{X}\in\mathbb{R}^{N_{\text{train}}\times d}$
la matriz de características (posiblemente estandarizadas) y
$\tilde{y}\in\mathbb{R}^{N_{\text{train}}}$ los valores transformados.
La Elastic Net resuelve, en esencia,
\[
  \min_{\beta_0,\beta}
    \frac{1}{N_{\text{train}}}
    \sum_{t\in\mathcal{I}_{\text{train}}}
      \bigl( \tilde{y}_t - \beta_0 - \tilde{X}_t^\top\beta \bigr)^2
    + \alpha\Bigl[
      (1-\eta)\frac{\|\beta\|_2^2}{2} + \eta\|\beta\|_1
    \Bigr],
\]
donde $\eta = \texttt{l1\_ratio}$ controla el balance entre
penalización $\ell_2$ (Ridge) y $\ell_1$ (Lasso).

\section{Split temporal y métricas de error ponderadas}

\subsection{Split 60/20/20}

\texttt{temporal\_split} y \texttt{\_segment\_indices} definen:

\begin{align*}
\mathcal{I}_{\text{train}} &= \{0,\dots,N_{\text{train}}-1\},\\
\mathcal{I}_{\text{val}}   &= \{N_{\text{train}},\dots,N_{\text{train}}+N_{\text{val}}-1\},\\
\mathcal{I}_{\text{test}}  &= \{N_{\text{train}}+N_{\text{val}},\dots,N-1\}.
\end{align*}

\subsection{RMSE no ponderado}

La función
\[
  \texttt{rmse}(y,\hat{y})
  = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}
\]
se calcula en cada segmento (train, val, test, train\_val) mediante
\texttt{compute\_segment\_metrics}.

\subsection{RMSE linealmente ponderado}

Para enfatizar los últimos días, definimos pesos
\[
  w_i \propto i+1, \quad i=0,\dots,n-1,
  \quad\text{con}\quad
  \sum_{i=0}^{n-1} w_i = 1.
\]
Entonces
\[
  \texttt{weighted\_rmse\_linear\_time}
  = \sqrt{ \sum_{i=0}^{n-1} w_i (y_i - \hat{y}_i)^2 }.
\]

Esto se usa:
\begin{itemize}
  \item como \emph{métrica de evaluación} para cada segmento,
  \item y también, más adelante, como esquema de \emph{entrenamiento}
        (usando \texttt{sample\_weight}) en el modo
        \texttt{train\_weight\_mode = "linear"}.
\end{itemize}

\subsection{RMSE exponencialmente ponderado}

Definimos pesos exponenciales en el tiempo,
controlando la razón ``último vs primero'':
\[
  \frac{w_{n-1}}{w_0} = R,\quad R = \texttt{ratio\_last\_first}.
\]
Esto induce
\[
  \gamma^{n-1} = R \quad\Rightarrow\quad \gamma = R^{1/(n-1)},
\]
y
\[
  w_i \propto \gamma^i,\quad i=0,\dots,n-1,\quad
  \sum_i w_i = 1.
\]
La métrica
\[
  \texttt{weighted\_rmse\_exponential\_time}
  = \sqrt{ \sum_{i=0}^{n-1} w_i (y_i - \hat{y}_i)^2 }
\]
asigna mucho más peso a los días más recientes.
De nuevo, se usa tanto como métrica de evaluación como,
en \texttt{\_fit\_and\_evaluate\_with\_train\_weight\_mode},
como esquema de \emph{entrenamiento} (\texttt{train\_weight\_mode = "exp"}).

\section{Espacio transformado vs espacio de precios}

\subsection{Métricas en el espacio transformado}

Cada vez que entrenamos un modelo,
\texttt{\_fit\_and\_evaluate} y
\texttt{\_fit\_and\_evaluate\_with\_train\_weight\_mode}
devuelven, para cada segmento $S\in\{\text{train},\text{val},\text{test},\text{train\_val}\}$:
\[
  \text{metrics}_S^{(y)} =
  \left\{
    \texttt{rmse},\;
    \texttt{rmse\_w\_linear},\;
    \texttt{rmse\_w\_exp}
  \right\},
\]
es decir, el error en el espacio de la transformación
$Y_t$ (log-precios, retornos, etc.).

\subsection{Métricas en el espacio original de precios}

Para poder comparar transformaciones entre sí en las unidades
originales (precio), se introdujo
\texttt{compute\_price\_space\_metrics\_for\_model}:

\begin{enumerate}
  \item Reconstruye $(X_{\text{all}},y_{\text{all}},\text{dates})$ para una transformación $g$ fija.
  \item Calcula $\hat{y}_t = f_{\hat{\theta}}(x_t)$ para todos los puntos.
  \item Aplica una \emph{detransformación} $h$ para aproximar $P_t$:
    \begin{itemize}
      \item \texttt{"close"}:
        $h(\hat{y}_t) = \hat{y}_t$.
      \item \texttt{"log\_close"}:
        $h(\hat{y}_t) = \exp(\hat{y}_t)$.
      \item \texttt{"sqrt\_close"}:
        $h(\hat{y}_t) = (\max\{\hat{y}_t,0\})^2$.
      \item \texttt{"zscore\_log\_close"}:
        $h(\hat{y}_t) = \exp(\hat{y}_t \sigma_{\log P} + \mu_{\log P})$.
      \item \texttt{"diff\_close"}:
        \begin{align*}
        \hat{P}_0 &= P_0,\\
        \hat{P}_t &= \hat{P}_{t-1} + \hat{y}_t,\quad t\ge 1.
        \end{align*}
      \item \texttt{"simple\_return"}:
        \begin{align*}
        \hat{P}_0 &= P_0,\\
        \hat{P}_t &= \hat{P}_{t-1}(1+\hat{y}_t),\quad t\ge 1.
        \end{align*}
      \item \texttt{"log\_return"}:
        \begin{align*}
        \widehat{\log P}_0 &= \log P_0,\\
        \widehat{\log P}_t &= \widehat{\log P}_{t-1} + \hat{y}_t,\\
        \hat{P}_t &= \exp(\widehat{\log P}_t).
        \end{align*}
    \end{itemize}
  \item Con $\hat{P}_t$ y $P_t$ alineados, se define para cada segmento $S$
        el mismo trío de métricas:
        \[
          \text{metrics}_S^{(P)} =
          \left\{
            \texttt{rmse},\;
            \texttt{rmse\_w\_linear},\;
            \texttt{rmse\_w\_exp}
          \right\},
        \]
        ahora en unidades de precio.
\end{enumerate}

Así, \texttt{run\_transform\_experiment} y
\texttt{run\_all\_transforms\_experiment} imprimen simultáneamente
métricas en el espacio transformado y en el espacio de precios.

\section{Experimentos de alto nivel}

\subsection{\texttt{run\_transform\_experiment}}

Para un \emph{solo} tipo de transformación (p.\,ej.\ \texttt{"log\_close"}):

\begin{enumerate}
  \item Descarga la serie \texttt{df} de Yahoo!.
  \item Construye $(X,y,\text{dates})$.
  \item Ajusta un modelo Elastic Net polinomial de grado
        \texttt{poly\_degree}.
  \item Calcula la varianza de $y$:
        \[
          \operatorname{Var}(Y) = \frac{1}{N}\sum_{t=0}^{N-1}
              (Y_t - \bar{Y})^2.
        \]
  \item Calcula métricas en el espacio transformado y en el espacio de precios,
        para train/val/test/train\_val.
  \item Devuelve:
        \begin{itemize}
          \item un diccionario \texttt{result} con:
                \begin{itemize}
                  \item \texttt{"transform"},
                  \item \texttt{"variance"},
                  \item \texttt{"metrics"},
                \end{itemize}
          \item el modelo ajustado,
          \item el \texttt{DataFrame} de precios.
        \end{itemize}
\end{enumerate}

\subsection{\texttt{run\_all\_transforms\_experiment}}

Para una lista de transformaciones
(\texttt{None} $\Rightarrow$ todas las de \texttt{TRANSFORM\_FUNCS}):

\begin{enumerate}
  \item Descarga una sola vez la serie de precios.
  \item Para cada transformación $g$:
    \begin{itemize}
      \item ajusta un modelo Elastic Net,
      \item almacena sus métricas en espacio transformado y de precios.
    \end{itemize}
  \item Imprime un resumen por transformación.
  \item Devuelve:
    \begin{itemize}
      \item \texttt{results[name]} con varianza y métricas,
      \item \texttt{models[name]} con el modelo para esa transformación,
      \item el \texttt{DataFrame} de precios.
    \end{itemize}
\end{enumerate}

\subsection{\texttt{run\_logprice\_experiment}}

Es un \emph{wrapper} de retrocompatibilidad que fija
\texttt{transform\_name = "log\_close"} y devuelve directamente
\texttt{result["metrics"]}, el modelo y el \texttt{DataFrame}.

\section{Entrenamiento ponderado en el tiempo}

\subsection{\texttt{\_fit\_and\_evaluate\_with\_train\_weight\_mode}}

Esta función extiende el entrenamiento para permitir
tres esquemas de peso en los \emph{datos de entrenamiento}:

\begin{itemize}
  \item \texttt{"plain"}: sin pesos (\texttt{sample\_weight = None}).
  \item \texttt{"linear"}: \texttt{sample\_weight} $\propto i+1$.
  \item \texttt{"exp"}: \texttt{sample\_weight} exponenciales con
        razón \texttt{ratio\_last\_first}.
\end{itemize}

Formalmente, en lugar del MSE simple
\[
  \frac{1}{N_{\text{train}}}
  \sum_{t\in\mathcal{I}_{\text{train}}}
    (y_t - \hat{y}_t)^2,
\]
minimizamos
\[
  \sum_{t\in\mathcal{I}_{\text{train}}}
    w_t (y_t - \hat{y}_t)^2,
\]
con $w_t$ definidos según el modo.

\subsection{\texttt{run\_weighted\_training\_comparison\_single\_transform}}

Para una transformación fija (p.\,ej.\ \texttt{"log\_close"}):

\begin{enumerate}
  \item Ajusta tres modelos (uno por modo de peso en entrenamiento).
  \item Para cada modo:
    \begin{itemize}
      \item calcula métricas en espacio transformado,
      \item calcula métricas en espacio de precios.
    \end{itemize}
  \item Imprime, en particular, las métricas de validación
        para comparar cuál esquema se comporta mejor.
\end{enumerate}

\subsection{\texttt{plot\_weighted\_training\_comparison}}

Genera una figura con:
\begin{itemize}
  \item panel izquierdo: serie transformada $Y_t$ y tres curvas de tendencia
        (plain / linear / exp) en el espacio de la transformación,
  \item panel derecho: precios $P_t$ y tres tendencias destransformadas.
\end{itemize}

\section{Diagnóstico de residuos y normalidad}

\subsection{\texttt{compute\_validation\_residuals\_transform\_and\_price}}

Para un modelo ajustado:

\begin{enumerate}
  \item Reconstruye $y_t$ y $\hat{y}_t$.
  \item Extrae solo el segmento de validación.
  \item Destransforma para obtener $P_t$ y $\hat{P}_t$ en validación.
  \item Devuelve:
    \begin{itemize}
      \item \texttt{resid\_y\_val} $= y_t - \hat{y}_t$,
      \item \texttt{resid\_p\_val} $= P_t - \hat{P}_t$,
      \item fechas y valores ajustados correspondientes.
    \end{itemize}
\end{enumerate}

\subsection{\texttt{compute\_normality\_stats}}

Dado un vector de residuos $\{e_i\}$, calcula:

\begin{align*}
\bar{e} &= \frac{1}{n}\sum_i e_i,\\
s^2    &= \frac{1}{n-1}\sum_i (e_i - \bar{e})^2,\\
\text{skew} &= \frac{1}{n}\sum_i
  \biggl(\frac{e_i - \bar{e}}{s}\biggr)^3,\\
\text{kurt\_excess} &= 
  \frac{1}{n}\sum_i
  \biggl(\frac{e_i - \bar{e}}{s}\biggr)^4 - 3.
\end{align*}

Además, calcula la estadística de Jarque--Bera:
\[
  \text{JB}
  = \frac{n}{6}\left(
      \text{skew}^2 +
      \frac{\text{kurt\_excess}^2}{4}
    \right),
\]
con $p$--valor bajo la aproximación $\chi^2_2$.

\subsection{\texttt{plot\_validation\_residual\_diagnostics}}

Para un modelo y transformación dados, puede trabajar en:

\begin{itemize}
  \item espacio transformado: residuos $e_t = y_t - \hat{y}_t$,
  \item espacio de precios: residuos $e_t = P_t - \hat{P}_t$.
\end{itemize}

Produce una figura $2\times 2$ con:
\begin{enumerate}
  \item Histograma + curva Normal ajustada.
  \item QQ-plot frente a una Normal.
  \item Residuos vs valores ajustados.
  \item Residuos vs fecha.
\end{enumerate}
Además, imprime $\bar{e}$, $s$, t--estadístico para $H_0:\bar{e}=0$,
$p$--valor, skewness y kurtosis-exceso.

\section{Selección de modelo regularizada por normalidad}

\subsection{\texttt{run\_normality\_regularized\_model\_single\_transform}}

Para una transformación fija (p.\,ej.\ \texttt{"log\_close"}) y una grilla de hiperparámetros:
\[
  \alpha\in\mathcal{A},\quad
  \eta\in\mathcal{L},\quad
  d\in\mathcal{D},\quad
  \text{modo}\in\{\text{plain, linear, exp}\},
\]
se exploran todas las combinaciones.

Para cada combinación se hace:

\begin{enumerate}
  \item Entrenamiento con el modo de peso escogido
        (\texttt{\_fit\_and\_evaluate\_with\_train\_weight\_mode}).
  \item Cálculo de métricas en el espacio de precios (en particular
        RMSE de validación $\mathrm{RMSE}^{(P)}_{\text{val}}$).
  \item Cálculo de residuos de validación en el espacio escogido
        (\texttt{normality\_space = "price"} o \texttt{"transform"}),
        y a partir de ellos la estadística JB.
  \item Definición de una función objetivo
        \[
          \text{obj}
          = w_{\text{rmse}}\cdot
            \text{RMSE}^{(P)}_{\text{val}}
          + w_{\text{jb}}\cdot
            \text{JB},
        \]
        y se almacena todo en un diccionario de candidatos.
\end{enumerate}

El ``mejor'' modelo es el que minimiza $\mathrm{obj}$.
Se devuelve:
\begin{itemize}
  \item \texttt{best\_summary}: hiperparámetros óptimos, métricas y
        estadísticas de normalidad,
  \item \texttt{best\_model}: el \texttt{Pipeline} entrenado,
  \item \texttt{all\_candidates}: detalles de todas las combinaciones,
  \item el \texttt{DataFrame} de precios.
\end{itemize}

\section{Visualizaciones de alto nivel}

\subsection{\texttt{plot\_transform\_trend} y \texttt{plot\_log\_trend}}

\begin{itemize}
  \item \texttt{plot\_transform\_trend} muestra la serie transformada
        $Y_t$ separada por colores (train/val/test) y la curva de tendencia
        ajustada (Elastic Net) sobre todo el rango temporal.
  \item \texttt{plot\_log\_trend} es un wrapper específico para
        \texttt{"log\_close"}.
\end{itemize}

\subsection{\texttt{plot\_price\_with\_exp\_trend}}

Es un caso particular para log-precios: toma un modelo entrenado
en \texttt{"log\_close"}, destransforma por exponencial y dibuja
la serie $P_t$ junto con la curva $\exp(\hat{y}_t)$
(separando colores por segmento).

\subsection{\texttt{plot\_all\_transform\_trends} y 
\texttt{plot\_all\_destransformed\_price\_trends}}

Para cada transformación y su modelo correspondiente:

\begin{itemize}
  \item \texttt{plot\_all\_transform\_trends} recorre 
        \texttt{transform\_names} y llama a
        \texttt{plot\_transform\_trend} para cada una.
  \item \texttt{plot\_all\_destransformed\_price\_trends} hace lo mismo
        pero en el espacio de precios, usando
        \texttt{plot\_destransformed\_price\_trend}.
\end{itemize}

\subsection{\texttt{subplot\_all\_transforms\_and\_destransforms}}

Construye una figura compacta en forma de rejilla:

\begin{itemize}
  \item Cada fila corresponde a una transformación.
  \item Columna izquierda: serie transformada $Y_t$ con su tendencia.
  \item Columna derecha: $P_t$ con la tendencia destransformada.
\end{itemize}

Esto permite ver de un vistazo cómo cambian la forma de la serie
y la suavidad de la tendencia según la transformación elegida.

\section{Resumen conceptual}

En síntesis, el módulo:

\begin{enumerate}
  \item Descarga una serie de precios y la convierte en un problema
        de regresión en el tiempo.
  \item Aplica múltiples transformaciones sobre el precio
        (niveles, log, raíz, diferencias, retornos, z-score).
  \item Ajusta modelos Elastic Net polinomiales en el índice temporal,
        con posibilidad de entrenar enfatizando más los datos recientes.
  \item Evalúa el ajuste tanto en el espacio transformado como
        en el espacio original de precios,
        usando RMSE no ponderado y dos variantes de RMSE ponderado.
  \item Genera figuras que muestran:
        \begin{itemize}
          \item la tendencia en el espacio transformado,
          \item la tendencia destransformada en el espacio de precios,
          \item la comparación de distintos esquemas de peso.
        \end{itemize}
  \item Analiza los residuos de validación (histograma, QQ-plot,
        residuo vs ajustado vs tiempo) y cuantifica su normalidad.
  \item Implementa una selección de modelo que combina
        ``buen desempeño predictivo'' (bajo RMSE de validación)
        con ``residuos lo más normales posible'' (bajo Jarque--Bera),
        explorando una grilla de hiperparámetros y modos de peso.
\end{enumerate}

En notación compacta, el corazón del enfoque puede resumirse como:
\[
  \boxed{
  \hat{\theta}
  = \arg\min_{\theta}
    \Bigl[
      \underbrace{
        \text{RMSE}^{(P)}_{\text{val}}(\theta)
      }_{\text{ajuste en precio}}
      +
      \lambda\,
      \underbrace{
        \text{JB}\bigl( e_{\text{val}}^{(*)}(\theta) \bigr)
      }_{\text{normalidad de residuos}}
    \Bigr]
  }
\]
donde $\theta$ codifica $(\alpha,\eta,d,\text{modo de peso})$,
$e_{\text{val}}^{(*)}$ son los residuos (en espacio de precios o transformado)
y $\lambda$ captura el peso relativo que damos a la normalidad frente al RMSE.




\newpage
\section*{Transformaciones, regresión y ``destransformación'' de la serie}

En esta sección explicamos con todo detalle qué está pasando en el módulo
\texttt{py103.py}, especialmente en el caso de la transformación por diferencias
(\texttt{diff\_close}), y aclaramos exactamente qué se está
``destransformando'' cuando dibujamos la serie en el espacio original de precios.

\subsection*{1. Notación básica}

\begin{itemize}
  \item Sea $t \in \{0,1,\dots,n-1\}$ el índice de tiempo discreto (días).
  \item Sea $P_t$ el precio de cierre (\texttt{close}) de NVDA en el día $t$.
  \item Sea $\mathcal{D} = \{(t,P_t)\}_{t=0}^{n-1}$ nuestra serie temporal.
  \item Sea $g(\cdot)$ una transformación escalar aplicada a $P_t$.
  \item Sea $y_t = g(P_{\bullet})$ el target transformado que realmente usamos en la regresión.
\end{itemize}

En el código, usamos varios $g$ distintos:
\begin{align*}
  \texttt{close}:       &\quad y_t = P_t,\\
  \texttt{log\_close}:  &\quad y_t = \log P_t,\\
  \texttt{diff\_close}: &\quad y_t = P_t - P_{t-1},\\
  \texttt{simple\_return}: &\quad y_t = \frac{P_t - P_{t-1}}{P_{t-1}},\\
  \texttt{log\_return}:    &\quad y_t = \log P_t - \log P_{t-1},
\end{align*}
etc.\ (para algunos $t$ iniciales, la serie se recorta o se rellena de forma
conveniente; en el código lo manejamos con máscaras y \texttt{dropna}).

\subsection*{2. Construcción del problema de regresión}

El modelo de regresión que usamos es un polinomio en el tiempo $t$,
regularizado con Elastic Net.

\subsubsection*{2.1. Variable explicativa: índice de tiempo}

Definimos el índice de tiempo
\[
  x_t = t \in \mathbb{R}, \qquad t = 0,1,\dots, n-1.
\]
En notación matricial,
\[
  X = 
  \begin{bmatrix}
    0\\
    1\\
    \vdots\\
    n-1
  \end{bmatrix}
  \in \mathbb{R}^{n \times 1}.
\]

Si usamos un polinomio de grado $d$ en $t$, internamente aplicamos
\texttt{PolynomialFeatures}:
\[
  \phi_d(t) = \bigl(t, t^2, \dots, t^d\bigr),
\]
y el diseño completo es
\[
  Z = \bigl[\phi_d(t_0)^\top;\; \phi_d(t_1)^\top;\; \dots;\; \phi_d(t_{n-1})^\top\bigr]
  \in \mathbb{R}^{n \times d}.
\]

\subsubsection*{2.2. Variable respuesta: transformación de la serie}

Dado un esquema de transformación $g$, construimos
\[
  y_t = g(P_{\bullet}), \qquad t \in \mathcal{I}_g,
\]
donde $\mathcal{I}_g \subseteq \{0,\dots,n-1\}$ es el conjunto de instantes
para los que la transformación está bien definida (por ejemplo, para
\texttt{diff\_close} empezamos en $t=1$ y ajustamos índices).

En forma vectorial escribimos
\[
  y = (y_t)_{t \in \mathcal{I}_g} \in \mathbb{R}^m,
\]
donde $m = |\mathcal{I}_g|$.

\subsubsection*{2.3. Separación en train / val / test}

Ordenando por tiempo, dividimos $y$ y $X$ en tres segmentos contiguos:
\begin{align*}
  \text{train: } & t = 0,\dots, n_{\mathrm{train}}-1, \\
  \text{val: }   & t = n_{\mathrm{train}},\dots, n_{\mathrm{train}}+n_{\mathrm{val}}-1, \\
  \text{test: }  & t = n_{\mathrm{train}}+n_{\mathrm{val}},\dots,n-1.
\end{align*}
En el código usamos proporciones
\[
  \texttt{train\_frac} = 0.6, \quad
  \texttt{val\_frac}   = 0.2, \quad
  \text{resto} = 0.2 \text{ para test}.
\]

\subsection*{3. Modelo de regresión en el espacio transformado}

El modelo es siempre una regresión lineal sobre las features polinomiales,
regularizada con Elastic Net. En notación estadística:

\[
  y_t = f_\theta(t) + \varepsilon_t, \qquad t \in \mathcal{I}_g,
\]
donde
\[
  f_\theta(t) = \beta_0 + \beta_1 t + \beta_2 t^2 + \dots + \beta_d t^d
\]
(es decir, el modelo es lineal en los parámetros $\beta$, pero no en $t$),
y $\varepsilon_t$ es el error.

El ajuste se hace resolviendo
\[
  \hat{\theta} =
  \underset{\theta}{\arg\min}\;\Bigg\{
    \sum_{t \in \text{train}}
    \bigl(y_t - f_\theta(t)\bigr)^2
    + \lambda\left[
      (1-\alpha)\|\theta\|_2^2 + \alpha\|\theta\|_1
    \right]
  \Bigg\},
\]
donde:
\begin{itemize}
  \item $\lambda = \texttt{alpha}$ en el código.
  \item $\alpha = \texttt{l1\_ratio}$ controla la mezcla Lasso/Ridge.
\end{itemize}

El modelo ajustado produce, para cada $t$,
\[
  \hat{y}_t = f_{\hat{\theta}}(t).
\]

\subsection*{4. Caso especial: transformación de diferencias}

Para \texttt{diff\_close}, definimos
\[
  D_t = P_t - P_{t-1}, \qquad t = 1,\dots,n-1.
\]

En la práctica:
\begin{itemize}
  \item Acomodamos los índices para que $y_t = D_t$ esté alineado con el
        conjunto de fechas correspondiente.
  \item Ajustamos el modelo
    \[
      D_t = f_\theta(t) + \varepsilon_t.
    \]
\end{itemize}

Tras ajustar, tenemos un estimador suave de las diferencias diarias:
\[
  \hat{D}_t = f_{\hat{\theta}}(t).
\]

\subsection*{5. ``Destransformación'': reconstruir un \emph{trend} en el espacio de precios}

El punto clave: \textbf{sólo destransformamos la salida del modelo
$\hat{y}_t$, no la serie observada $y_t$}.

\subsubsection*{5.1. Transformaciones directas (niveles, logaritmos, etc.)}

Cuando $g$ es biyectiva punto a punto $P_t \mapsto y_t$, por ejemplo:

\begin{itemize}
  \item \texttt{close}: $y_t = P_t$, entonces $h(y_t) = y_t$.
  \item \texttt{log\_close}: $y_t = \log P_t$, entonces $h(y_t) = e^{y_t}$.
\end{itemize}

Definimos una inversa $h$ tal que
\[
  h(g(P_t)) = P_t.
\]

En este caso, el \emph{trend} de precios que dibujamos es
\[
  \hat{P}_t = h(\hat{y}_t),
\]
y la banda de confianza en el espacio de precios es
\[
  \hat{P}_t^{\text{low}}  = h\bigl(\hat{y}_t + q_{\mathrm{low}}\bigr),\qquad
  \hat{P}_t^{\text{high}} = h\bigl(\hat{y}_t + q_{\mathrm{high}}\bigr),
\]
donde $q_{\mathrm{low}},q_{\mathrm{high}}$ son cuantil(es) empíricos de los
residuos en el \emph{espacio transformado}.

\subsubsection*{5.2. Transformaciones acumulativas (diferencias, rendimientos)}

Para \texttt{diff\_close}, el transform es
\[
  y_t = D_t = P_t - P_{t-1}.
\]

Ya no existe una inversa punto a punto $h$ que dependa sólo de $y_t$; la
información de $P_t$ está codificada \emph{acumulativamente} en todos los
incrementos previos. La relación verdadera es
\[
  P_t = P_0 + \sum_{s=1}^t D_s.
\]

En el modelo, sustituimos $D_s$ por el valor ajustado $\hat{D}_s$:
\[
  \hat{P}_t
  = P_0 + \sum_{s=1}^t \hat{D}_s
  = P_0 + \sum_{s=1}^t f_{\hat{\theta}}(s).
\]

En código se implementa como recursión:
\begin{align*}
  \hat{P}_0 &= P_0,\\
  \hat{P}_t &= \hat{P}_{t-1} + \hat{D}_t,
  \qquad t = 1,\dots,n-1.
\end{align*}

Esto produce un \textbf{trend suave} en el espacio de precios:
\[
  \hat{P}_t = \text{``precio suavizado''}
\]
porque $\hat{D}_t$ es una función suave de $t$ (polinomio regularizado).

Análogamente, si usamos una banda empírica en el espacio de diferencias:
\[
  \hat{D}_t^{\mathrm{low}} = \hat{D}_t + q_{\mathrm{low}},\qquad
  \hat{D}_t^{\mathrm{high}} = \hat{D}_t + q_{\mathrm{high}},
\]
reconstruimos las bandas de precio por
\begin{align*}
  \hat{P}_0^{\mathrm{low}} &= P_0,\qquad
  \hat{P}_t^{\mathrm{low}} = \hat{P}_{t-1}^{\mathrm{low}} + \hat{D}_t^{\mathrm{low}},\\
  \hat{P}_0^{\mathrm{high}} &= P_0,\qquad
  \hat{P}_t^{\mathrm{high}} = \hat{P}_{t-1}^{\mathrm{high}} + \hat{D}_t^{\mathrm{high}}.
\end{align*}

\paragraph{Conclusión importante.}
En el caso \texttt{diff\_close}:

\begin{itemize}
  \item Nunca reconstruimos $P_t$ usando los verdaderos incrementos $D_t$ en
        la gráfica de \emph{trend}.
  \item Siempre usamos los \emph{incrementos ajustados} $\hat{D}_t$ para
        obtener una curva suave $\hat{P}_t$.
  \item La serie observada $P_t$ se dibuja tal cual, con sus brincos; la
        curva $\hat{P}_t$ es un suavizado.
\end{itemize}

Por eso, visualmente, la línea negra (trend) es continua y suave, mientras que
la serie de puntos/segmentos de la serie cruda puede ser más ``salto a salto''.

\subsection*{6. Qué se destransforma y qué no}

Podemos resumir así:

\begin{enumerate}
  \item \textbf{Espacio transformado.}
    \begin{itemize}
      \item Datos observados: $y_t = g(P_{\bullet})$.
      \item Modelo: $\hat{y}_t = f_{\hat{\theta}}(t)$.
      \item Residuos: $r_t = y_t - \hat{y}_t$.
    \end{itemize}
  \item \textbf{Espacio de precios (destransformado).}
    \begin{itemize}
      \item Serie observada: $P_t$ se dibuja tal cual (\texttt{df["close"]}).
      \item \textbf{Sólo destransformamos:}
        \begin{itemize}
          \item La curva de trend $\hat{y}_t \mapsto \hat{P}_t$.
          \item Las bandas de confianza $\hat{y}_t + q \mapsto \hat{P}_t^{(\cdot)}$.
        \end{itemize}
    \end{itemize}
\end{enumerate}

En particular, nunca hacemos
\[
  y_t \xrightarrow{h} P_t^\star
\]
para luego sustituir $P_t$ por $P_t^\star$; es decir, no ``destransformamos'' la
serie observada, sólo la salida del modelo.

\subsection*{7. Ejemplo compacto: \texttt{diff\_close}}

Para fijar ideas, el pipeline de \texttt{diff\_close} es:

\begin{align*}
  D_t &= P_t - P_{t-1},\\
  D_t &= f_\theta(t) + \varepsilon_t,\\
  \hat{D}_t &= f_{\hat{\theta}}(t),\\
  \hat{P}_0 &= P_0,\\
  \hat{P}_t &= \hat{P}_{t-1} + \hat{D}_t.
\end{align*}

Visualmente:

\begin{itemize}
  \item En la gráfica de \textbf{transformación} (\texttt{diff\_close}):
    \[
      \boxed{D_t \text{ observado (salta)} \quad vs \quad \hat{D}_t \text{ suave}.}
    \]
  \item En la gráfica de \textbf{precio}:
    \[
      \boxed{P_t \text{ observado (salta)} \quad vs \quad
      \hat{P}_t = P_0 + \sum_{s=1}^t \hat{D}_s \text{ suave}.}
    \]
\end{itemize}

\subsection*{8. Resumen simbólico}

La idea central que explica tu observación sobre la línea continua es:

\[
  \boxed{
    \begin{aligned}
      & \text{(1) Transformamos la serie: } y_t = g(P_{\bullet}). \\
      & \text{(2) Ajustamos } y_t \approx f_\theta(t)\ \Rightarrow\ \hat{y}_t. \\
      & \text{(3) Construimos un \emph{trend} en precios: }
        \hat{P}_t = H(\hat{y}_{0:t}),\\
      &\quad\text{donde } H \text{ integra o invierte } g \text{ usando sólo la salida del modelo.}
    \end{aligned}
  }
\]

En el caso de diferencias, $H$ es la suma acumulada de incrementos ajustados;
como éstos son suaves, la curva $\hat{P}_t$ es también suave y no reproduce los
saltos originales de $P_t$, sino su tendencia estimada.

\end{document}
